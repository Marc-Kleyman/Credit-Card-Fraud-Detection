---
title: 'Analyzing Open LLM Performance'
author: "Team MMA - Mohamed Bakr, Marc Kleyman, Ankit Dey"
date: "2023-10-19"
output:
  bookdown::pdf_document2:
    toc: true
    citation_package: natbib
    fig_caption: true
header-includes:
  - \usepackage{parskip}
---

```{=tex}
\newpage
\setcounter{page}{1}
```
```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(stargazer)
library(sandwich)
library(knitr)
library(patchwork)
library(kableExtra)
library(car)
library(lmtest)

# install.packages("caret")
library(caret)

theme_set(theme_bw())

knitr::opts_chunk$set(echo = FALSE, include=FALSE, message = FALSE, fig.pos = "H", out.extra = "")
set.seed(123)
```

```{r load data}
data <- read_csv("../data/raw/Open LLM models performance benchmarks.csv")
```

```{r}
original_count <- nrow(data)
n_na <- sum(is.na(data$Type))
clean_data <- data %>% drop_na(Type)
n_no_na <- nrow(clean_data)
n_unique <- length(unique(clean_data$model_name_for_query))
n_duplicate <- sum(table(clean_data$model_name_for_query) > 1)
```

## Introduction {.unnumbered}

With the release of ChatGPT and the surge in AI adoption, many companies are searching for feasible large language model solutions to solve their problems. Many small-cap and mid-cap companies are unable to dedicate the resources necessary to work on this endeavor themselves, so they are seeking help from some of the top consulting firms in the nation. An industry research study performed by Future Market Insights shows that the AI Consulting industry has a market size of roughly \$525 billion in 2023[^1] and will most likely continue to grow.

[^1]: Global AI Consulting Services Market Outlook (2023 to 2033).[Future Market Insights](https://www.futuremarketinsights.com/reports/ai-consulting-services-market/)

As employees of one of the leading AI Consulting firms, our team of machine learning consultants was tasked with building a large language model for a mid-cap customer that we want to do recurring business with. To ensure our firm was showcasing its best work, we decided to conduct this study to determine which model was the best and if the tuning type was the key to its success (for replicability). To explore this idea, we formulated the following research question:

```{=tex}
\begin{quote}
  \textit{“How does changing the tuning type of a Large Language Model (LLM) cause a change in the quality of its outputs?”}
\end{quote}
```
Below is our initial causal path diagram for our research:

<!-- $$ -->

<!-- \text{Tuning Type} \\ -->

<!-- \swarrow\nearrow \;\;\;\;\;\; \downarrow \;\;\;\;\;\;\\ -->

<!-- \text{No. Parameters} \rightarrow \text{Avg. Score} \leftarrow \text{Precision} \\ -->

<!-- \;\;\;\;\;\; \updownarrow \;\;\;\;\;\;\\ -->

<!-- \epsilon -->

<!-- $$ -->

<center>

![Initial Causal Path Diagram](initial%20causal%20path%20diagram.png){width="35%"}

</center>

## Data and Methodology {.unnumbered}

In our analysis, we utilize Open LLM Performance Benchmark data[^2], which is a survey of `r original_count` Comprehensive Language Model Evaluation Metrics that is updated from the HuggingFace leader board[^3] as of October 16th, 2023. HuggingFace is a platform and open-source provider of machine learning technologies. It allows users to share datasets and machine learning models, and showcase their work. The HuggingFace Open LLM Leaderboard tracks and ranks evaluations of LLMs using benchmarks from the Eleuther AI Language Model Evaluation Harness. The unit of observation in this dataset is one singular LLM. The goal of this analysis is to determine which LLM is best for inference and are varying tuning `Type` (X) to have a positive impact on `Average Score` (Y). We can only present one model to the client, so we cannot use multiple models or parts of a model as our unit of observation.

[^2]: Open LLM Performance Benchmark.[Open LLM performance Dataset](https://www.kaggle.com/datasets/itsraghul/open-llm-performance-dataset/)

[^3]: Hugging Face Leaderboard. [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/)

```{r, Explore-Confirm-Spit}
# The Exploration subsample percentage
explore_percentage <- 0.7
```

We performed all exploration and model building on a `r explore_percentage * 100`% subsample of the data. The remaining `r (1 - explore_percentage) * 100`%, totaling `r round(n_no_na * (1 - explore_percentage), 0)` rows, was used to generate the statistics in this report.

Tuning type is operationalized by the X variable `Tuning Type`, which is a nominal variable with four classes: Instruction-Tuned, Fine-Tuned, Pretrained, and Reinforcement Learning-Tuned. The X concept refers to the weight calibration method that was used to guide the large language model's training after it was pretrained on a dataset. `Tuning Type` was one-hot encoded to calculate correlations and allow for identification of OVB. Model quality is operationalized by the Y variable `Average score`, which is an average score (between 1 and 100) across four benchmarks that measure different aspects of the quality of an LLM's outputs. Each LLM goes through all four benchmarks and the benchmarks each measure a different aspect of the quality of the LLM's outputs: `ARC` (question-answering abilities), `HellaSwag` (common sense reasoning), `MMLU` (language understanding), and `TruthfulQA` (truthfulness). The scores in each benchmark are metric and continuous and are determined by giving the large language model a series of questions and calculating the percentage of questions the model gets correct. The average score for each LLM is the simply the mean of the four percentage scores.


```{r make summary table}
# Create the data frame
accounting_table <- data.frame(
  Cause = c("Start", "Missing Tuning Type", "Exploration subsample"),
  Available = c(nrow(data), n_no_na, round(n_no_na * (1 - explore_percentage), 0)),
  Remove = c(NA, n_na, round(n_no_na * explore_percentage, 0))
)
```

```{r Accounting-table, fig.pos='H', include=TRUE}
kable(
  accounting_table,
  col.names = c("Cause", "Number of Samples Available For Analysis (after removal for cause)", "Number of Samples Removed"),
  digits = 3,
  caption = "Accounting Table", 
  booktabs = TRUE,
) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(1, width = "15em") %>% 
  column_spec(2, width = "15em") %>%
  column_spec(3, width = "10em")

```

As we report in Table \@ref(tab:Accounting-table), we removed `r n_na` models with missing `Type`, leaving `r n_no_na` models. Each row represents a single LLM. Each LLM is evaluated on the test dataset of each benchmark, and the residuals are aggregated within each benchmark to compute the score for that benchmark for that LLM. There are `r n_unique` unique LLMs in the dataset, with a `r n_duplicate` LLMs being benchmarked repeatedly.

```{r assess-multicolinearity-assumption, include=FALSE, echo=FALSE}
#install.packages("corrplot")
library(corrplot)

data_clean <- drop_na(data)

# Take the sample with the best score for duplicates
data_clean <- data_clean %>%
  group_by(model_name_for_query) %>%
  slice_max(order_by = `Average score`, n = 1) %>%
  ungroup()

# One hot encode 'Type' and 'Precision' columns
data_one_hot <- data_clean %>%
  mutate(across(c(Type, Precision), factor)) %>%
  select(Type, Precision)

# Convert OHE columns to df
encoded_cols <- model.matrix(~ . - 1, data = data_one_hot) %>%
  as.data.frame()

# Select numeric columns
numeric_data <- select_if(data_clean, is.numeric)

# Combine numeric columns with one-hot encoded columns
combined_data <- bind_cols(numeric_data, encoded_cols)
```

```{r train-test-split, include=FALSE, echo=FALSE}
# Split the combined data into train and test sets
set.seed(123) 
split <- createDataPartition(combined_data$`Average score`, p = explore_percentage, list = FALSE)
train_data <- combined_data[split, ]
test_data <- combined_data[-split, ]
```

We are interested in the effect of changing tuning type `Tuning Type` on the average score the model receives across all four benchmarks `Average Score`. The current scientific consensus suggests that the relationship between score and the number of parameters (the only metric co-variate that we consider) may be linear or close to linear within the range of parameters considered in the data. We therefore create regression models with no transformations applied to the output or predictor variables. In other words, we fit regressions of the form,

$$
  \widehat{Avg. Score}=\beta_0 + \beta_1 \cdot (Fine\ Tuned) + \beta_2 \cdot (Instruction\ Tuned) + \beta_3 \cdot (RL\ Tuned)+ \mathbf{Z\gamma}
$$

Where $\beta_1$ represents the predicted change in average score when changing from Pretrained to Fine Tuned, $\beta_2$ represents the predicted change in average score when changing from Pretrained to Instruction Tuned, $\beta_3$ represents the predicted change in average score when changing from Pretrained to RL-Tuned, $\mathbf{Z}$ is a row vector of additional covariates, and $\mathbf{\gamma}$ is a column vector of predicted coefficients for these covariates.

We considered specifications that included the score of each LLM on individual benchmarks (`ARC`, `HellaSwag`, `TruthfulQA`, or `MMLU`), however these are outcome variables that are directly related to average score `Average Score`, and are therefore not appropriate to include as predictors. We also considered specifications that include a nominal variable that represents the base model used prior to tuning, derived from the model name variable `model_name_for_query`. The method for deriving this variable required matching up pieces of the names of LLMs to identify LLMs with the same base model, which was unreliable and therefore not used. We explored two other covariates called `Params` (parameters) and `Precision`. `Params` is the number of estimated coefficients in the model, which is mathematically known to impact the score -- LLMs with a higher number of `Params` usually perform better on benchmarks. The number of parameters may also be correlated with the tuning type because it may be too expensive to use certain tuning types on a model with more parameters.

```{r corr-plot, fig.cap="Correlation Matrix Plot", include=FALSE}
# Compute correlation matrix
correlation_matrix_train <- cor(train_data)

corrplot(correlation_matrix_train, method = "circle")
```

For `Precision`, encoding floats as torch.bfloat16 (brain floats) over torch.float16 is known to be associated with an increase in inference accuracy, and therefore an increase in score, due to the increased range of brain floats (Grobbelaar, 2023). Additionally, precision type may be correlated with the tuning type -- it is possible that some forms of tuning do not work well with the decreased range of regular 16-bit floats.

## Results {.unnumbered}

```{r create-models}
# Define the model formulas
formula1 <- `Average score` ~ `Typefine-tuned` + `Typeinstruction-tuned` + `TypeRL-tuned` + `Typepretrained`
formula2 <- update(formula1, . ~ . + `#Params (B)`)
formula3 <- update(formula2, . ~ . + `Precision8bit` + `PrecisionGPTQ` + `Precisiontorch.bfloat16` + `Precisiontorch.float16` + `Precisiontorch.float32`)
```

```{r, include=FALSE, echo=FALSE}
# Fit the models
model1 <- lm(formula1, data = train_data)
se_model1 <- model1 %>% 
  vcovHC(type = "HC1") %>% 
  diag() %>% 
  sqrt()

model2 <- lm(formula2, data = train_data)
se_model2 <- model2 %>% 
  vcovHC(type = "HC1") %>% 
  diag() %>% 
  sqrt()

model3 <- lm(formula3, data = train_data)
se_model3 <- model3 %>% 
  vcovHC(type = "HC1") %>% 
  diag() %>% 
  sqrt()

# Summarize the models
summary(model1)
summary(model2)
summary(model3)
```

<!-- Evaluating the **IID** assumption: -->

<!-- Evaluating the **No Perfect Colinearity** assumption -->

<!-- R has not dropped any variables when checking the model's coefficients. This tells us that there is no perfect collinearity. and from the plot it looks the no perfect colinearity assumption passes there are couple of high values in the vif test of model 3 though for the `Precisiontorch.bfloat16`  and `Precisiontorch.float16` not really sure what it means-->

```{r}
models = list(model1, model2, model3)
model_names <- c("Model 1", "Model 2", "Model 3")

# Extract coefficients and model names
coefficients <- rbind(model1$coefficients, model2$coefficients, model3$coefficients)

# Add model names as a new column
coefficients <- cbind(model_names, coefficients)

# Display the coefficients in a table
kable(coefficients, caption = "Model Coefficients", format = "markdown")

```

```{r}
formula1 <- `Average score` ~ `Typefine-tuned` + `Typeinstruction-tuned` + `TypeRL-tuned`
formula2 <- update(formula1, . ~ . + `#Params (B)`)
formula3 <- update(formula2, . ~ . + `Precision8bit` + `PrecisionGPTQ` + `Precisiontorch.bfloat16` + `Precisiontorch.float16` + `Precisiontorch.float32`)

model1vif <- lm(formula1, data = train_data)
se_model1 <- model1 %>% 
  vcovHC(type = "HC1") %>% 
  diag() %>% 
  sqrt()

model2vif <- lm(formula2, data = train_data)
se_model2 <- model2 %>% 
  vcovHC(type = "HC1") %>% 
  diag() %>% 
  sqrt()

model3vif <- lm(formula3, data = train_data)
se_model3 <- model3 %>% 
  vcovHC(type = "HC1") %>% 
  diag() %>% 
  sqrt()

vifs <- rbind(vif(model1vif), vif(model2vif), vif(model3vif))
vifs <- cbind(model_names, vifs)
kable(vifs, caption = "Model variance inflation factor", format = "markdown")
```

```{r}
# Function to add model predictions and residuals with custom names
add_model_predictions <- function(data, model, model_name) {
  
  # Add new columns with custom names
  data <- data %>%
    mutate(
      !!paste0(model_name, "_preds") := predict(model),
      !!paste0(model_name, "_resids") := resid(model)
    )
  
  # Return the updated data frame
  return(data)
}

train_data <- add_model_predictions(train_data, model1, "model1")
train_data <- add_model_predictions(train_data, model2, "model2")
train_data <- add_model_predictions(train_data, model3, "model3")

```

```{r pair-plot, fig.cap="Pairwise Correlation Plot", message = FALSE}
# Function to add model predictions and residuals to data and create scatterplots
pair_plot <- function(data, columns) {
  # Create and display scatterplots
  GGally::ggpairs(data %>% select(columns))
}
model1_columns <- c("Average score", "Typefine-tuned", "Typeinstruction-tuned", "TypeRL-tuned", "Typepretrained")
model2_columns <- c(model1_columns, "#Params (B)")
model3_columns <- c(model2_columns, "Precision8bit", "PrecisionGPTQ", "Precisiontorch.bfloat16", "Precisiontorch.float16", "Precisiontorch.float32")
model1_pair_plot <- pair_plot(train_data, model1_columns)
model2_pair_plot <- pair_plot(train_data, model2_columns)
model3_pair_plot <- pair_plot(train_data, model3_columns)

model1_pair_plot
model2_pair_plot
model3_pair_plot

```

<!-- Evaluating the **Linear Conditional Expectation** assumption -->

<!-- the Linear Conditional Expectation assumption seems to be violated for the tree models specially in model 3-->

<!-- Evaluating the **Homoskedastic Errors** assumption -->

<!-- The Breusch-Pagan  p-value is below 0.05, only for model 2 and 3 which tells us that we can reject the null hypothesis and assume that the errors have non-constant variance (heteroskedasticity). model 1 p-value is above 0.05 we can't reject the null hypothesis there is no enough evidence for heteroskedasticity The same can be observed in the scale-location plot -->

```{r}
bptest(model1)
bptest(model2)
bptest(model3)
```

```{r scale-location-plot, fig.cap="scale-location plot"}
plot(model1, which=3)
plot(model2, which=3)
plot(model3, which=3)
```

<!-- Evaluating the **Normal Distribution of Errors** Assumption -->

<!-- residuals seems not to follow a normal distribution histogram for all three models have about3 modes QQ plot show skewness -->

```{r}
# Function to create residual plots
plot_residuals <- function(data, model, model_name) {
  # Extract model residuals
  model_resids <- resid(model)

  # Create plot 1: Residuals histogram
  plot_one <- ggplot(data, aes(x = model_resids)) +
    geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black", alpha = 0.7) +
    labs(
      title = paste0("Histogram of the Model Residuals (", model_name, ")"),
      x = "Model Residuals",
      y = "Frequency"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5)  # Center the plot title
    )

  # Create plot 2: Q-Q plot
  plot_two <- ggplot(data, aes(sample = model_resids)) +
    stat_qq() + stat_qq_line(color = "red") +
    labs(
      title = paste0("Q-Q Residuals (", model_name, ")"),
      x = "Theoretical Quantiles",
      y = "Standardized Residuals"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5)  # Center the plot title
    )

  # Combine the plots
  plot_one / plot_two
}

# Example usage
plot_residuals(train_data, model1, "Model 1") | plot_residuals(train_data, model2, "Model 2") | plot_residuals(train_data, model3, "Model 3")

```
```{r}
# # Generate predictions for the test data
# test_data$pred_model1 <- predict(model1, newdata = test_data)
# test_data$pred_model2 <- predict(model2, newdata = test_data)
# test_data$pred_model3 <- predict(model3, newdata = test_data)
# 
# # Create new response variables which are the residuals (actual - predicted)
# test_data$resid_model1 <- test_data$`Average score` - test_data$pred_model1
# test_data$resid_model2 <- test_data$`Average score` - test_data$pred_model2
# test_data$resid_model3 <- test_data$`Average score` - test_data$pred_model3
# 
# # Fit new models on the residuals
# resid_model1 <- lm(resid_model1 ~ `Typefine-tuned` + `Typeinstruction-tuned` + `TypeRL-tuned` + `Typepretrained`, data = test_data)
# resid_model2 <- lm(resid_model2 ~ `Typefine-tuned` + `Typeinstruction-tuned` + `TypeRL-tuned` + `Typepretrained` + `#Params (B)`, data = test_data)
# resid_model3 <- lm(resid_model3 ~ `Typefine-tuned` + `Typeinstruction-tuned` + `TypeRL-tuned` + `Typepretrained` + `#Params (B)` + `Precision8bit` + `PrecisionGPTQ` + `Precisiontorch.bfloat16` + `Precisiontorch.float16` + `Precisiontorch.float32`, data = test_data)

```

```{r create-stargazer-table, echo=FALSE, include=FALSE, results='asis'}
# Function to calculate test statistics including Adjusted R^2 and F-statistic
calculate_test_stats <- function(model, test_data) {
    predictions <- predict(model, newdata = test_data)
    residuals <- test_data[["Average score"]] - predictions

    rss <- sum(residuals^2)
    tss <- sum((test_data[["Average score"]] - mean(test_data[["Average score"]]))^2)
    n <- nrow(test_data)
    p <- length(coef(model)) - 1  # Number of predictors excluding intercept

    r_squared <- 1 - rss / tss
    residual_std_error <- sqrt(rss / (n - p - 1))
    
    # Calculate adjusted R^2
    adj_r_squared <- 1 - (1 - r_squared) * ((n - 1) / (n - p - 1))
    
    # Calculate F-statistic
    f_statistic <- (tss - rss) / p / (rss / (n - p - 1))
    
    # Calculate degrees of freedom
    df1 <- p
    df2 <- n - p - 1
    p_value <- 1 - pf(f_statistic, df1, df2)
    
    if (is.nan(r_squared) || is.infinite(r_squared)) {
        r_squared <- NA  # Handle NaN or Inf
        adj_r_squared <- NA
        f_statistic <- NA
        df1 <- NA
        df2 <- NA
        p_value <- NA
    }

    return(list(R_Squared = round(r_squared, 2), Adj_R_Squared = round(adj_r_squared, 2), 
                F_Statistic = round(f_statistic, 2), Residual_Std_Error = round(residual_std_error, 2),
                DF1 = df1, DF2 = df2, P_Value = signif(p_value, digits=3)))
}



# Calculate train statistics for each model using the training data
train_model1 <- calculate_test_stats(model1, train_data)
train_model2 <- calculate_test_stats(model2, train_data)
train_model3 <- calculate_test_stats(model3, train_data)

# Calculate test statistics
test_model1 <- calculate_test_stats(model1, test_data)
test_model2 <- calculate_test_stats(model2, test_data)
test_model3 <- calculate_test_stats(model3, test_data)

# Create lists for train and test statistics
train_R_squared <- c(train_model1$R_Squared, train_model2$R_Squared, train_model3$R_Squared)
train_adj_R_squared <- c(train_model1$Adj_R_Squared, train_model2$Adj_R_Squared, train_model3$Adj_R_Squared)
train_F_statistic <- c(train_model1$F_Statistic, train_model2$F_Statistic, train_model3$F_Statistic)
train_residual_std_error <- c(train_model1$Residual_Std_Error, train_model2$Residual_Std_Error, train_model3$Residual_Std_Error)

test_R_squared <- c(test_model1$R_Squared, test_model2$R_Squared, test_model3$R_Squared)
test_adj_R_squared <- c(test_model1$Adj_R_Squared, test_model2$Adj_R_Squared, test_model3$Adj_R_Squared)
test_F_statistic <- c(test_model1$F_Statistic, test_model2$F_Statistic, test_model3$F_Statistic)
test_residual_std_error <- c(test_model1$Residual_Std_Error, test_model2$Residual_Std_Error, test_model3$Residual_Std_Error)

# Create the Stargazer table
stargazer_table <- stargazer(model1, model2, model3, type = "latex", header = F,
                             title = "Comparison of Linear Models",
                             dep.var.caption = "Output Variable:",
                             dep.var.labels = "Average LLM Score",
                             column.labels = c("Model 1", "Model 2", "Model 3"),
                             intercept.bottom = FALSE,
                             digits = 2,
                             star.cutoffs = c(0.05, 0.01, 0.001),
                             single.row = TRUE,
                             add.lines = list(
                                 c("Test Observations", rep(nrow(test_data), 3)),
                                 c("Test R-Squared", test_R_squared),
                                 c("Test Adjusted R-Squared", test_adj_R_squared),
                                 c("Test Residual Std. Error", test_residual_std_error),
                                 c("Test Degrees of Freedom", c(test_model1$DF2, test_model2$DF2, test_model3$DF2)),
                                 c("Test F-Statistic", test_F_statistic),
                                 c("Test F-Statistic P-Value", c(test_model1$P_Value, test_model2$P_Value, test_model3$P_Value))
                             ),
                             covariate.labels = c("Constant", "Type (Fine Tuned)", "Type (Instruction Tuned)", "Type (RL Tuned)", "Type (Pretrained)", "No. of Parameters (B)",
                       "Precision (8 bit)", "Precision (GPTQ)", "Precision (torch.bfloat16)", "Precision (torch.float16)", "Precision (torch.float32)"),
                             out = "table.tex")


```

![Regression Table and Residuals vs. Fitted Plot](combined_plots.png){width="100%"}

```{r Linear-conditional-expectation-plot, fig.cap="Residual vs Fitted plot", include=TRUE, fig.width=4, fig.height=3, fig.align="center"}
# Function to create a residual vs fitted plot
residual_plot <- function(data, model, model_name) {
  # Extract model predictions and residuals
  model_preds <- predict(model)
  model_resids <- resid(model)

  # Create the plot
  ggplot(data, aes(x = model_preds, y = model_resids)) +
    geom_point() +
    stat_smooth() +
    labs(
      x = "Fitted Values",
      y = "Residuals",
      title = paste0("Residuals vs Fitted (", model_name, ")")
    ) +
    theme(
      plot.title = element_text(hjust = 0.5)  # Center the plot title
    )
}

#residual_plot(train_data, model1, "Model 1")
#residual_plot(train_data, model2, "Model 2")
res_plot <- residual_plot(train_data, model3, "Model 3")
```


```{r, include=FALSE}
summary_model3 <- summary(model3)
coefficients_table_model3 <- summary_model3$coefficients
print(coefficients_table_model3)
p_value_precision8bit <- round(coefficients_table_model3["Precision8bit", "Pr(>|t|)"], 3)
coef_precision8bit <- round(coefficients_table_model3["Precision8bit", "Estimate"], 2)
coef_params <- round(coefficients_table_model3["`#Params (B)`", "Estimate"], 2)
p_value_params <- round(coefficients_table_model3["`#Params (B)`", "Pr(>|t|)"], 3)

coef_fine <- round(coefficients_table_model3["`Typefine-tuned`", "Estimate"], 2)
coef_instruction <- round(coefficients_table_model3["`Typeinstruction-tuned`", "Estimate"], 2)
coef_rl <- round(coefficients_table_model3["`TypeRL-tuned`", "Estimate"], 2)
p_value_fine <- round(coefficients_table_model3["`Typefine-tuned`", "Pr(>|t|)"], 3)
p_value_instruction <- round(coefficients_table_model3["`Typeinstruction-tuned`", "Pr(>|t|)"], 3)
p_value_rl <- round(coefficients_table_model3["`TypeRL-tuned`", "Pr(>|t|)"], 3)
se_instruction_tuned <- round(summary_model3$coefficients["`Typeinstruction-tuned`", "Std. Error"], 2)
```

```{r compare-model-1-and-2-coefs}
# Extract coefficients and standard errors for both models
coef_model1 <- summary(model1)$coefficients
se_model1 <- coef(summary(model1))[, "Std. Error"]

coef_model2 <- summary(model2)$coefficients
print(coef_model2)
se_model2 <- coef(summary(model2))[, "Std. Error"]

# Tuning category variables
tuning_categories <- c("`Typefine-tuned`", "`Typeinstruction-tuned`", "`TypeRL-tuned`")

comparison_results <- data.frame(
  Category = character(),
  Diff_Coef = numeric(),
  Combined_SE = numeric(),
  T_Score = numeric(),
  P_Value = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each tuning category and perform comparison
for (cat in tuning_categories) {
  diff_coef <- coef_model2[cat, "Estimate"] - coef_model1[cat, "Estimate"]
  combined_se <- sqrt(se_model1[cat]^2 + se_model2[cat]^2)
  t_score <- diff_coef / combined_se
  df <- length(coef_model1) + length(coef_model2) - 2  # degrees of freedom
  p_value <- 2 * (1 - pt(abs(t_score), df))
  
  # Append results to the data frame
  comparison_results <- rbind(comparison_results, data.frame(Category = cat, Diff_Coef = diff_coef, Combined_SE = combined_se, T_Score = t_score, P_Value = p_value))
}

comparison_results$Diff_Coef <- round(comparison_results$Diff_Coef, 2)
comparison_results$Combined_SE <- round(comparison_results$Combined_SE, 2)
comparison_results$T_Score <- round(comparison_results$T_Score, 2)
comparison_results$P_Value <- round(comparison_results$P_Value, 3)

# Print the results
print(comparison_results)

```

```{r}
fine_tuned_t_score <- comparison_results["`Typefine-tuned`", "T_Score"]
fine_tuned_p_value <- comparison_results["`Typefine-tuned`", "P_Value"]
```

```{r contrast-tests, include=FALSE}
library(multcomp)
# 
# # Define the contrasts for comparisons
# contrasts <- rbind(
#     "Instruction vs RL" = c("`Typeinstruction-tuned`" = 1, "`TypeRL-tuned`" = -1, "`Typefine-tuned`" = 0, `Typepretrained` = 0),
#     "RL vs Fine" = c("`Typeinstruction-tuned`" = 0, "`TypeRL-tuned`" = 1, "`Typefine-tuned`" = -1, `Typepretrained` = 0),
#     "Instruction vs Fine" = c("`Typeinstruction-tuned`" = 1, "`TypeRL-tuned`" = 0, "`Typefine-tuned`" = -1, `Typepretrained` = 0)
# )
# 
# # Perform the tests
# results <- glht(model3vif, linfct = mcp(contrasts = contrasts))
# summary(results)

```

All of the one hot encoded tuning categories were found to have statistically significant effects across all 3 models, supporting the idea that tuning type has a causal impact on score. The addition of number of parameters as a predictor in model 2 led to a statistically significant change in the regression coefficient for the fine-tuned category (T = `r fine_tuned_t_score`, p = `r fine_tuned_p_value`), representing a reduction in bias for that coefficient. The test F-statistic for model 2 (F = `r test_model2$F_Statistic`, p = `r test_model2$P_Value`) is greater than that of model 3 (F = `r test_model3$F_Statistic`, p = `r test_model3$P_Value`), indicating that the addition of precision predictor variables in model 3 does not lead to a proportionately better fit. However, since at least one of the regression coefficients for precision was found to be statistically significant (8-bit precision, $\beta_{8bit}$ = `r round(model3$coefficients["Precision8bit"], 2)`, p = `r p_value_precision8bit`), we recommend model 3 as a basis for estimating causal effects and making modeling decisions. 8-bit precision is the only precision category with a statistically significant coefficient. The negative effect of changing to 8-bit precision from another precision type on predicted average score is large compared to the positive effect of adding another billion parameters to the model ($\beta_{8bit}$ = `r coef_precision8bit`, $\beta_{params}$ = `r coef_params`), which makes sense since 8-bit precision covers the smallest range of numbers out of all of the precision types. For context, we would need to remove around 32.5 billion parameters from the model to produce the same negative effect on predicted average score as switching from torch.float16 to 8-bit precision, holding all else constant. Thus, we recommend against using 8-bit precision in the LLM to be developed for the client. The low p-value for $\beta_{params}$ (p = `r p_value_params`) indicates a high degree of confidence in the positive impact of increasing the number of parameters on average score. To put this in context, we would need to add around 36 billion parameters to the model to produce the same positive impact on predicted average score as switching from a pre-training only strategy to instruction tuning, holding all else constant. Thus, we recommend developing a larger model for the client -- 70 billion parameters is frequently chosen to maximize inference performance without being too costly, however, this should be adjusted based on use case, as some specific tasks may perform well with less parameters. The coefficients for the fine-tuned ($\beta_{fine-tuned}$ = `r coef_fine`, p = `r p_value_fine`), instruction-tuned ($\beta_{instruction-tuned}$ = `r coef_instruction`, p = `r p_value_instruction`), and RL-tuned ($\beta_{RL-tuned}$ = `r coef_rl`, p = `r p_value_rl`) categories are all relatively large and positive, indicating a significant positive impact of tuning on average score as compared to only pre-training. The instruction-tuned category seems to have the greatest positive impact on average score since it has the largest coefficient with small standard error (Std. Error = `r se_instruction_tuned`), so we recommend the development team to use instruction tuning or a combination of tuning strategies that includes instruction-based tuning to produce the best performing LLM for the client.

## Limitations {.unnumbered}

Regarding IID assumptions, the identical distribution assumption may be challenged since the LLMs aren't required to share a common architecture. The independence assumption may be challenged since LLMs built by the same developer may have similar scores, but we removed any duplicate LLMs with the same model name. Overall, we can assume IID is violated.

The VIF for all coefficients were between 1 and 2 except for 16-bit floats and 16-bit bfloats, which have a VIF of roughly 9. This indicates that there is no multicollinearity except with 16-bit floats and 16-bit bfloats. Because the VIF is greater than 4, we can assume there is moderate collinearity.

To check for linear conditional expectation, we created a plot of residuals vs. predicted values (Figure 2). Because the line is nonlinear and away from zero, we can assume that the linear conditional expectation assumption is violated.

To check for normality, we plotted the residuals on a Q-Q plot and a histogram. Both show that the data is close to following a normal distribution, but slightly heavy-tailed. Therefore, we can assume that the normality assumption is satisfied.

A studentized Breusch-Pagan test was run to test for homoscedasticity. The result was a p-value of 0.04, meaning the model has evidence of heteroscedasticity and violates the homoscedasticity assumption.

To check for zero conditional mean, we created a plot of residuals vs fitted values. The data created a non-linear lowess smoothing line fluctuating from -8 to 5. Because this line is not horizontal and near 0, we can assume that the zero conditional mean assumption is violated. 

The four LLM performance benchmark tests may not perfectly encapsulate what the "best model" is due to the types of questions asked, but we do not have access to these models' performance on other benchmark tests. We would like to have seen some benchmark tests reflecting quantitative reasoning through math or coding questions for a more comprehensive examination to determine the "best model". The pre-training data set for each model was not given and this omitted variable may have an impact on the score. We also considered using the model name as a potential covariate because it includes the underlying base model and may hint towards the architecture and pre-training data, but it may be difficult to parse that information with enough accuracy from the model name.

<center>

![Updated Causal Path Diagram](Updated%20causal%20path%20diagram.png){width="35%"}

</center>

## Conclusion {.unnumbered}

Our research demonstrates the positive impact of all tuning types for optimizing model performance while Instruction Tuning shows the most significant impact. We recommend building a larger model with more parameters since increasing the number of parameters had a positive impact on the average score. Additionally, we recommend avoiding 8-bit precision due to its negative influence on the Average Score.

Moving forward, we believe exploring tuning strategies inspired by instruction or combined tuning strategies are promising avenues for future research.
